{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7b6f69929b9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0manimal\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Zodiac'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatch_zodiac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Year'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatetimeIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from bokeh.charts import Scatter, Bar, show, output_notebook\n",
    "output_notebook()\n",
    "data = pd.read_csv('../input/3-Airplane_Crashes_Since_1908.txt',sep=',')\n",
    "data.sample()\n",
    "\n",
    "# Return a bunch of tuples with the Zodiac and its Start/End Dates\n",
    "def chinese_zodaics():\n",
    "    start_date = pd.to_datetime(\"2/2/1908\")\n",
    "    end_date = pd.to_datetime(\"7/1/2009\")\n",
    "    animals = ['Monkey', 'Rooster', 'Dog', 'Pig', 'Rat', 'Ox', 'Tiger', 'Rabbit', 'Dragon', 'Snake', 'Horse', 'Goat']\n",
    "    zodiacs = []\n",
    "    while start_date < end_date:\n",
    "        for a in animals:    \n",
    "            year_start = start_date\n",
    "            year_end = year_start + pd.DateOffset(days=365)\n",
    "            z = (a, start_date, year_end)\n",
    "            zodiacs.append(z)\n",
    "            start_date = year_end\n",
    "    return zodiacs \n",
    "\n",
    "zodiacs = chinese_zodaics()\n",
    "\n",
    "# Apply the zodiacs to the accident dates\n",
    "def match_zodiac(date):\n",
    "    for z in zodiacs: \n",
    "        animal, start, end, = z[0], z[1], z[2]\n",
    "        if start <= date <= end:\n",
    "            return animal\n",
    "        \n",
    "data.Date = pd.to_datetime(data.Date)\n",
    "data['Zodiac'] = data.Date.apply(match_zodiac)\n",
    "data['Year'] = pd.DatetimeIndex(data['Date']).year\n",
    "data = data[['Zodiac', 'Year', 'Fatalities', 'Aboard']].dropna()\n",
    "data = data[data.Fatalities > 1]\n",
    "data.sample(5)\n",
    "# Return a bunch of tuples with the Zodiac and its Start/End Dates\n",
    "def chinese_zodaics():\n",
    "    start_date = pd.to_datetime(\"2/2/1908\")\n",
    "    end_date = pd.to_datetime(\"7/1/2009\")\n",
    "    animals = ['Monkey', 'Rooster', 'Dog', 'Pig', 'Rat', 'Ox', 'Tiger', 'Rabbit', 'Dragon', 'Snake', 'Horse', 'Goat']\n",
    "    zodiacs = []\n",
    "    while start_date < end_date:\n",
    "        for a in animals:    \n",
    "            year_start = start_date\n",
    "            year_end = year_start + pd.DateOffset(days=365)\n",
    "            z = (a, start_date, year_end)\n",
    "            zodiacs.append(z)\n",
    "            start_date = year_end\n",
    "    return zodiacs \n",
    "\n",
    "zodiacs = chinese_zodaics()\n",
    "\n",
    "# Apply the zodiacs to the accident dates\n",
    "def match_zodiac(date):\n",
    "    for z in zodiacs: \n",
    "        animal, start, end, = z[0], z[1], z[2]\n",
    "        if start <= date <= end:\n",
    "            return animal\n",
    "        \n",
    "data.Date = pd.to_datetime(data.Date)\n",
    "data['Zodiac'] = data.Date.apply(match_zodiac)\n",
    "data['Year'] = pd.DatetimeIndex(data['Date']).year\n",
    "data = data[['Zodiac', 'Year', 'Fatalities', 'Aboard']].dropna()\n",
    "data = data[data.Fatalities > 1]\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])], [frozenset([1, 3]), frozenset([2, 5]), frozenset([2, 3]), frozenset([3, 5])], [frozenset([2, 3, 5])], []]\n",
      "{frozenset([5]): 0.75, frozenset([3]): 0.75, frozenset([2, 3, 5]): 0.5, frozenset([3, 5]): 0.5, frozenset([2, 3]): 0.5, frozenset([2, 5]): 0.75, frozenset([1]): 0.5, frozenset([1, 3]): 0.5, frozenset([2]): 0.75}\n",
      "frozenset([3]) , frozenset([1]) S1Upport 0.75 conf11: 0.666666666667\n",
      "frozenset([1]) , frozenset([3]) S1Upport 0.5 conf11: 1.0\n",
      "frozenset([5]) , frozenset([2]) S1Upport 0.75 conf11: 1.0\n",
      "frozenset([2]) , frozenset([5]) S1Upport 0.75 conf11: 1.0\n",
      "frozenset([3]) , frozenset([2]) S1Upport 0.75 conf11: 0.666666666667\n",
      "frozenset([2]) , frozenset([3]) S1Upport 0.75 conf11: 0.666666666667\n",
      "frozenset([5]) , frozenset([3]) S1Upport 0.75 conf11: 0.666666666667\n",
      "frozenset([3]) , frozenset([5]) S1Upport 0.75 conf11: 0.666666666667\n",
      "frozenset([5]) , frozenset([2, 3]) S1Upport 0.75 conf11: 0.666666666667\n",
      "frozenset([3]) , frozenset([2, 5]) S1Upport 0.75 conf11: 0.666666666667\n",
      "frozenset([2]) , frozenset([3, 5]) S1Upport 0.75 conf11: 0.666666666667\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Dec 04 22:25:57 2013\n",
    "\n",
    "@author: Administrator\n",
    "\"\"\"\n",
    "import codecs\n",
    "\n",
    "def loadDataSet():\n",
    " return [[1,3,4],[2,3,5],[1,2,3,5],[2,5]]\n",
    " f = codecs.open(\"d:\\\\exercise\\\\associationrules.txt\", encoding='utf-16')\n",
    "#content = f.readlines()\n",
    "#print content\n",
    " result = list()  \n",
    " Array1=[]\n",
    " ArraySub=[]\n",
    " str_prevLine=''\n",
    " i=0\n",
    " for line in f.readlines():\n",
    "#    line = line.strip()\n",
    "    #print line\n",
    "    \n",
    "    \n",
    "    if not len(line) or line.startswith('#'):       #判断是否是空行或注释行  \n",
    "        continue                                    #是的话，跳过不处理  \n",
    "    parts = line.split(',')\n",
    "    #print parts\n",
    "    if str_prevLine==parts[0] :\n",
    "       ArraySub.append(parts[1])\n",
    "    else:\n",
    "       Array1.append(ArraySub)\n",
    "       ArraySub=[]\n",
    "       ArraySub.append(parts[1])\n",
    "   \n",
    "    #Array1.append(ArraySub)\n",
    "    col1=parts[0]\n",
    "    str_prevLine=parts[0]\n",
    "    i=i+1\n",
    "    \n",
    " Array1.append(ArraySub)\n",
    " return Array1\n",
    "def createC1(dataSet):#产生单个item的集合\n",
    "    C1=[]\n",
    "    for transaction in dataSet:\n",
    "        for item in transaction: \n",
    "            if not [item] in C1:\n",
    "                C1.append([item])\n",
    "    \n",
    "    C1.sort()\n",
    "    \n",
    "    return map(frozenset,C1)#给C1.list每个元素执行函数\n",
    "    \n",
    "    \n",
    "def scanD(D,ck,minSupport):#dataset,a list of candidate set,最小支持率\n",
    "    ssCnt={}\n",
    "    for tid in D:\n",
    "        for can in ck:\n",
    "            if can.issubset(tid):\n",
    "                if not ssCnt.has_key(can):\n",
    "                    ssCnt[can]=1\n",
    "                else: ssCnt[can]+=1\n",
    "    \n",
    "    numItem=float(len(D))\n",
    "    retList=[]\n",
    "    supportData={}\n",
    "    for key in ssCnt:\n",
    "        support=ssCnt[key]/numItem\n",
    "        if support>=minSupport:\n",
    "            retList.insert(0,key)\n",
    "            supportData[key]=support\n",
    "            \n",
    "    return retList,supportData#返回频繁k项集，相应支持度\n",
    "        \n",
    "\n",
    "def aprioriGen(Lk,k):#create ck(k项集)\n",
    "    retList=[]\n",
    "    lenLk=len(Lk)\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i+1,lenLk):\n",
    "            L1=list(Lk[i])[:k-2];L2=list(Lk[j])[:k-2]\n",
    "            L1.sort();L2.sort()#排序\n",
    "            if L1==L2:#比较i,j前k-1个项若相同，和合并它俩\n",
    "                retList.append(Lk[i] | Lk[j])#加入新的k项集 | stanf for union\n",
    "    return retList\n",
    "    \n",
    "    \n",
    "def apriori(dataSet,minSupport=0.5):\n",
    "    C1=createC1(dataSet)\n",
    "    D=map(set,dataSet)\n",
    "    L1,supportData=scanD(D,C1,minSupport)#利用k项集生成频繁k项集（即满足最小支持率的k项集）\n",
    "    L=[L1]#L保存所有频繁项集\n",
    "    \n",
    "    k=2\n",
    "    while(len(L[k-2])>0):#直到频繁k-1项集为空\n",
    "        Ck=aprioriGen(L[k-2],k)#利用频繁k-1项集 生成k项集\n",
    "        Lk,supK= scanD(D,Ck,minSupport)\n",
    "        supportData.update(supK)#保存新的频繁项集与其支持度\n",
    "        L.append(Lk)#保存频繁k项集\n",
    "        k+=1\n",
    "    return L,supportData#返回所有频繁项集，与其相应的支持率\n",
    "        \n",
    "    \n",
    "def calcConf(freqSet,H,supportData,brl,minConf=0.7):\n",
    "    prunedH=[]\n",
    "    for conseq in H:#后件中的每个元素\n",
    "        conf=supportData[freqSet]/supportData[freqSet-conseq]\n",
    "        if conf>=minConf:\n",
    "            print freqSet-conseq,',',conseq,'S1Upport',supportData[freqSet-conseq],'conf11:',conf\n",
    "            brl.append((freqSet-conseq,conseq,conf))#添加入规则集中\n",
    "            prunedH.append(conseq)#添加入被修剪过的H中\n",
    "        #print freqSet-conseq\n",
    "    return prunedH\n",
    "\n",
    "def rulesFromConseq(freqSet,H,supportData,brl,minConf=0.7):\n",
    "     \n",
    "    m=len(H[0])#H是一系列后件长度相同的规则，所以取H0的长度即可\n",
    "    if (len(freqSet)>m+1):\n",
    "        Hmp1=aprioriGen(H,m+1)\n",
    "        Hmp1=calcConf(freqSet,Hmp1,supportData,brl,minConf)\n",
    "        if (len(Hmp1)>1):\n",
    "            rulesFromConseq(freqSet,Hmp1,supportData,brl,minConf)\n",
    "            \n",
    "def generateRules(L,supportData,minConf=0.7):\n",
    "  \n",
    "    bigRuleList=[]#存储规则\n",
    "    for i in range(1,len(L)):\n",
    "        for freqSet in L[i]:\n",
    "            H1=[frozenset([item]) for item in freqSet]\n",
    "            if(i>1):\n",
    "                rulesFromConseq(freqSet,H1,supportData,bigRuleList,minConf)\n",
    "            else:\n",
    "                calcConf(freqSet,H1,supportData,bigRuleList,minConf)\n",
    "    return bigRuleList\n",
    "\n",
    "dataSet=loadDataSet()\n",
    "\n",
    "C1=createC1(dataSet)\n",
    "\n",
    "D=map(set,dataSet)\n",
    "L1,suppData0=scanD(D,C1,0.5)\n",
    "\n",
    "L,suppData=apriori(dataSet)\n",
    "\n",
    "print L\n",
    "print suppData\n",
    "rules=generateRules(L,suppData,minConf=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#import apriori #meant to be stored as a module file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "print 88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code to load transction into numpy list\n",
    "\n",
    "I need to get the output into a 2d array, with each person as a row, and the product as columns. The date will be dropped, and the values are summed.\n",
    "\n",
    "person, product, date, val\n",
    "A, x, 1/1/2013, 10\n",
    "A, x, 1/10/2013, 10\n",
    "B, x, 1/2/2013, 20\n",
    "B, y, 1/4/2013, 15\n",
    "A, y, 1/8/2013, 20\n",
    "C, z, 2/12/2013, 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [u' item_id\\r\\n'], [u' \\u83dc1\\r\\n', u'sku2meatB\\r\\n'], [u'Sku1vegB\\r\\n', u'Sku3vegC\\r\\n', u'Sku5vegC\\r\\n'], [u'Sku1\\r\\n'], [u'sku2\\r\\n', u'sku2\\r\\n'], [u'sku1\\r\\n', u'sku3']]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#coding=utf-8\n",
    "\n",
    "#with open(infile, \"d:\\\\exercise\\alibba search log.txt\") as f:\n",
    "#    for line in f:\n",
    "#        if \"\\n\" in line:\n",
    "#            line = line.replace(\"\\n\", \" \")\n",
    "\n",
    "import codecs\n",
    "f = codecs.open(\"d:\\\\exercise\\\\associationrules.txt\", encoding='utf-16')\n",
    "#content = f.readlines()\n",
    "#print content\n",
    "result = list()  \n",
    "Array1=[]\n",
    "ArraySub=[]\n",
    "str_prevLine=''\n",
    "i=0\n",
    "for line in f.readlines():\n",
    "#    line = line.strip()\n",
    "    #print line\n",
    "    \n",
    "    \n",
    "    if not len(line) or line.startswith('#'):       #判断是否是空行或注释行  \n",
    "        continue                                    #是的话，跳过不处理  \n",
    "    parts = line.split(',')\n",
    "    #print parts\n",
    "    if str_prevLine==parts[0] :\n",
    "       ArraySub.append(parts[1])\n",
    "    else:\n",
    "       Array1.append(ArraySub)\n",
    "       ArraySub=[]\n",
    "       ArraySub.append(parts[1])\n",
    "   \n",
    "    #Array1.append(ArraySub)\n",
    "    col1=parts[0]\n",
    "    str_prevLine=parts[0]\n",
    "    i=i+1\n",
    "    \n",
    "Array1.append(ArraySub)\n",
    "print Array1\n",
    "print isinstance(u'中文',unicode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-16-d100a57ef107>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-d100a57ef107>\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    for line in infile:\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import os\n",
    "import assoc\n",
    "\n",
    "\n",
    "#read in data to a dict object - sums scripts by tuple (doc, drug)\n",
    "dictObj = {}\n",
    "rawData = 'subset.txt'\n",
    "with open(rawData) as infile:\n",
    "for line in infile:\n",
    "    parts = line.split(',')\n",
    "    key = (parts[0],parts[1])\n",
    "    val = float(parts[3])\n",
    "    if key in dictObj:\n",
    "        dictObj[key] += val\n",
    "    else:\n",
    "        dictObj[key] = val\n",
    "infile.close()\n",
    "\n",
    "print \"stage 1 done\"\n",
    "#get the number of doctors and the number of drugs\n",
    "keys =  dictObj.keys()\n",
    "docs = list(set([x[0] for x in keys]))\n",
    "drugs = sorted(list(set([x[1] for x in keys])))\n",
    "\n",
    "#read through the dict and build out a 2d numpy array \n",
    "docC = 0\n",
    "mat = np.empty([len(docs),len(drugs)])\n",
    "for doc in docs:\n",
    "drugC = 0\n",
    "for drug in drugs:\n",
    "    key = (doc,drug)\n",
    "    if key in dictObj:\n",
    "        mat[(docC,drugC)] = dictObj[(key)]\n",
    "            else:\n",
    "        mat[(docC,drugC)] = 0\n",
    "    drugC += 1\n",
    "docC+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
